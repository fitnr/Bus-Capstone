{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This project parses 3 TB nested Json file into csv using pyspark along with sparksql for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention !! All the data used is test data for only partial data from Dec 30,2015. For complete version, please check other files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Json, parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from scipy.interpolate import interp1d # import dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bus_file='test.jsons'\n",
    "bus = sqlContext.read.json(bus_file)\n",
    "bus.registerTempTable(\"bus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bus.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load and apply SQL Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"spark_extract.sql\") as fr:\n",
    "     query = fr.read()\n",
    "output = sqlContext.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def extract(parts):\n",
    "    for p in parts:\n",
    "        for o in itertools.izip(p.Line,p.Latitude,p.Longitude,p.RecordedAtTime,p.vehicleID,p.Trip,p.TripDate):\n",
    "            yield o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_list(p):\n",
    "    if p.ROUTE_ID!=None:\n",
    "        return zip(p.ROUTE_ID,p.latitude,p.longitude,p.recorded_time\\\n",
    "                   ,p.vehicle_id,p.TRIP_ID,p.tripdate,p.SHAPE_ID\\\n",
    "                   ,p.STOP_ID,p.distance_stop,p.distance_shape,p.status,p.destination)\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranfer time to Unix time for interpolatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import dateutil.parser\n",
    "def unix_time(x):\n",
    "    dt = dateutil.parser.parse(x)\n",
    "    return time.mktime(dt.timetuple())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findIncreasingList(parts):\n",
    "    prev = 0\n",
    "    for record in parts:\n",
    "        if record[-1]<prev:\n",
    "            return\n",
    "        prev = record[-1]\n",
    "        yield record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d # import dependency\n",
    "def predict(x):\n",
    "    pre_x = [p[-1] for p in x if p[-1]!=None]\n",
    "    if len(pre_x) >= 2:\n",
    "        pre_y = [unix_time(p[1]) for p in x if p[-1]!=None]\n",
    "        f = interp1d(pre_x, pre_y)\n",
    "    else:\n",
    "        return []\n",
    "    return findIncreasingList([(p[0],p[2],p[3],f(p[-1]+p[-2]))\\\n",
    "                               for p in x if p[-1]!=None and (p[-1]+p[-2]) <= pre_x[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## method b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "def predict_map(x):\n",
    "    train_y = [unix_time(p[3]) for p in x if p[-3]!=None ]\n",
    "    if len(train_y) >= 2:\n",
    "        train_x = [p[-3] for p in x if p[-3]!=None]\n",
    "        f = interp1d(train_x, train_y)\n",
    "        distance = [(p[-3]+p[-4]) for p in x \\\n",
    "                    if p[-3]!=None and (p[-3]+p[-4]) <= train_x[-1]]\n",
    "        stoptimes = f(distance)\n",
    "        stops = [p[-5] for p in x if p[-3]!=None]\n",
    "    else:\n",
    "        return[]\n",
    "    return map(lambda a,b: (a,b), stops,stoptimes)\n",
    "    #return [(p[-4],f(p[-2]+p[-3])) for p in x if (p[-2]!=None and p[-3]!=None) and (p[-2]+p[-3]) <= pre_x[-1]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby Date and Line & Apply Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output.flatMap(parse_list)\\\n",
    "      .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group By TRIP_ID and Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.resultiterable.ResultIterable at 0x112c96d10>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112c96c90>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112c96150>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112c96a10>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112c96f90>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112c96e10>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112c95a10>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112c959d0>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112c95c90>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112c95050>]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.flatMap(parse_list)\\\n",
    "      .map(lambda x:((x[5],x[6]),x)).groupByKey()\\\n",
    "      .map(lambda x: x[1])\\\n",
    "      .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupbykey and Apply Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'MTA NYCT_Q46',\n",
       "  u'MTA NYCT_QV_W5-Weekday-110900_Q46_6',\n",
       "  u'MTA_502327',\n",
       "  array(1451520016.346892)),\n",
       " (u'MTA NYCT_Q46',\n",
       "  u'MTA NYCT_QV_W5-Weekday-110900_Q46_6',\n",
       "  u'MTA_502331',\n",
       "  array(1451520119.0)),\n",
       " (u'MTA NYCT_Q20A',\n",
       "  u'MTA NYCT_CS_W5-Weekday-113900_MISC_769',\n",
       "  u'MTA_505023',\n",
       "  array(1451520122.033537)),\n",
       " (u'MTA NYCT_Q20A',\n",
       "  u'MTA NYCT_CS_W5-Weekday-113900_MISC_769',\n",
       "  u'MTA_505024',\n",
       "  array(1451520122.0558827)),\n",
       " (u'MTA NYCT_Q20A',\n",
       "  u'MTA NYCT_CS_W5-Weekday-113900_MISC_769',\n",
       "  u'MTA_505026',\n",
       "  array(1451520338.3900702)),\n",
       " (u'MTA NYCT_BX8',\n",
       "  u'MTA NYCT_WF_W5-Weekday-110800_BX8_13',\n",
       "  u'MTA_100947',\n",
       "  array(1451520014.054865)),\n",
       " (u'MTA NYCT_BX8',\n",
       "  u'MTA NYCT_WF_W5-Weekday-110800_BX8_13',\n",
       "  u'MTA_103181',\n",
       "  array(1451520125.8062081)),\n",
       " (u'MTA NYCT_BX8',\n",
       "  u'MTA NYCT_WF_W5-Weekday-110800_BX8_13',\n",
       "  u'MTA_103183',\n",
       "  array(1451520377.7902856)),\n",
       " (u'MTA NYCT_BX10',\n",
       "  u'MTA NYCT_KB_W5-Weekday-111000_BX10_32',\n",
       "  u'MTA_103960',\n",
       "  array(1451519992.9190516)),\n",
       " (u'MTA NYCT_BX10',\n",
       "  u'MTA NYCT_KB_W5-Weekday-111000_BX10_32',\n",
       "  u'MTA_103966',\n",
       "  array(1451520201.7349436))]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.flatMap(parse_list)\\\n",
    "      .map(lambda x:((x[5],x[6]),(x[0],x[3],x[5],x[8],x[-4],x[-3])))\\\n",
    "      .groupByKey()\\\n",
    "      .flatMap(lambda x: predict(x[1]))\\\n",
    "      .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the prefix, timezones and save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output.flatMap(parse_list)\\\n",
    "      .map(lambda x:((x[5],x[6]),(x[0],)).groupByKey() \\\n",
    "      .flatMap(lambda x: predict(x[1]))\\\n",
    "      .map(lambda x: \",\".join(map(str, x)))\\\n",
    "      .map(lambda x: x.replace('MTA NYCT_', '').replace('MTABC_','').replace('MTA_','').replace('-05:00',''))\\\n",
    "      .saveAsTextFile('stoptimes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read From CSV and SQL Manupilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Schemas and Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "customSchema = StructType([StructField(\"ROUTE_ID\", StringType(), True),\\\n",
    "                           StructField(\"latitude\", DoubleType(), True),\\\n",
    "                           StructField(\"longitude\", DoubleType(), True),\\\n",
    "                           StructField(\"recorded_time\", StringType(), True),\\\n",
    "                           StructField(\"vehicle_id\", StringType(), True),\\\n",
    "                           StructField(\"TRIP_ID\", StringType(), True),\\\n",
    "                           StructField(\"tripdate\", DateType(), True),\\\n",
    "                           StructField(\"SHAPE_ID\", StringType(), True),\\\n",
    "                           StructField(\"STOP_ID\", StringType(), True),\\\n",
    "                           StructField(\"distance_stop\", StringType(), True),\\\n",
    "                           StructField(\"distance_shape\", StringType(), True),\\\n",
    "                           StructField(\"status\", StringType(), True),\\\n",
    "                           StructField(\"destination\", StringType(), True)])             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_times_schema = StructType([StructField(\"trip_id\", StringType(), True),\\\n",
    "                           StructField(\"arrival_time\", StringType(), True),\\\n",
    "                           StructField(\"departure_time\", StringType(), True),\\\n",
    "                           StructField(\"stop_id\", StringType(), True),\\\n",
    "                           StructField(\"stop_sequence\", IntegerType(), True),\\\n",
    "                           StructField(\"pickup_type\", IntegerType(), True),\n",
    "                           StructField(\"drop_off_type\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_stoptimes_schema = StructType([StructField(\"ROUTE_ID\", StringType(), True),\\\n",
    "                           StructField(\"TRIP_ID\", StringType(), True),\\\n",
    "                           StructField(\"STOP_ID\", StringType(), True),\\\n",
    "                           StructField(\"time\",IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use CSV=>DF tool to read saved csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_stoptimes = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('stops.csv', schema = real_stoptimes_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stoptimes = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('stop_times.txt',schema = stop_times_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stoptimes.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "record = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('all.csv', schema = customSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+----------+--------------------+----------+--------------------+----------+--------+-------+-------------+--------------+-------+-----------+\n",
      "|ROUTE_ID| latitude| longitude|       recorded_time|vehicle_id|             TRIP_ID|  tripdate|SHAPE_ID|STOP_ID|distance_stop|distance_shape| status|destination|\n",
      "+--------+---------+----------+--------------------+----------+--------------------+----------+--------+-------+-------------+--------------+-------+-----------+\n",
      "|     B67|40.664322|-73.983724|2015-12-30T18:59:...|       406|JG_W5-Weekday-109...|2015-12-30| B670106| 305679|        12.05|        8002.2|at stop|     801044|\n",
      "+--------+---------+----------+--------------------+----------+--------------------+----------+--------+-------+-------------+--------------+-------+-----------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "record.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "record.registerTempTable('record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_time = real_stoptimes.withColumn('realtime',split(pyspark.sql.functions.from_unixtime(real_stoptimes.time), ' ')[1])\\\n",
    "                         .withColumn('date',split(pyspark.sql.functions.from_unixtime(real_stoptimes.time), ' ')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------+----------+--------+----------+\n",
      "|ROUTE_ID|             TRIP_ID|STOP_ID|      time|realtime|      date|\n",
      "+--------+--------------------+-------+----------+--------+----------+\n",
      "|     Q46|QV_W5-Weekday-110...| 502327|1451520016|19:00:16|2015-12-30|\n",
      "|     Q46|QV_W5-Weekday-110...| 502331|1451520119|19:01:59|2015-12-30|\n",
      "+--------+--------------------+-------+----------+--------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_time.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_time.registerTempTable('new_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stoptimes.registerTempTable('stoptimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "def get_sec(s):\n",
    "    l = s.split(':')\n",
    "    return int(l[0]) * 3600 + int(l[1]) * 60 + int(l[2])\n",
    "sqlContext.registerFunction(\"getsec\", lambda x: get_sec(x), IntegerType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply getsec function to sec and calculate the delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "join = sqlContext.sql('SELECT ROUTE_ID,TRIP_ID,STOP_ID,realtime,date,(getsec(realtime)-getsec(arrival_time)) as delay\\\n",
    "                       FROM new_time\\\n",
    "                       INNNER JOIN stoptimes\\\n",
    "                       ON (TRIP_ID = trip_id AND STOP_ID = stop_id)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------+--------+----------+-----+\n",
      "|ROUTE_ID|             TRIP_ID|STOP_ID|realtime|      date|delay|\n",
      "+--------+--------------------+-------+--------+----------+-----+\n",
      "|     B41|FB_W5-Weekday-109...| 303241|19:03:19|2015-12-30|  754|\n",
      "|     B49|FB_W5-Weekday-110...| 303985|19:06:26|2015-12-30| 1046|\n",
      "+--------+--------------------+-------+--------+----------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "join.registerTempTable('new_join')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the performance on 3 ways:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. On Time performance: if the bus arrives 1min ahead of the schedule or 5mins after the schedule. It is ontime\n",
    "2. Peakhour wait assesment:if the bus arrives 3min ahead or 3min after the scheduled time on 6-9 or 16-19. It is ontime\n",
    "3. off-peak hour wait assesement: if the bus arrives within 5mins of the schedule except peak hours. It is ontime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+----------+------------+---------+---------+\n",
      "|ROUTE_ID|STOP_ID|      date|ontime_ratio|peak_wait|peak_wait|\n",
      "+--------+-------+----------+------------+---------+---------+\n",
      "|     B47| 306158|2015-12-30|         0.0|      0.0|      1.0|\n",
      "|      B9| 300956|2015-12-30|         0.0|      0.0|      1.0|\n",
      "|     B63| 306945|2015-12-30|         0.0|      0.0|      1.0|\n",
      "|      B8| 300692|2015-12-30|         0.0|      0.0|      1.0|\n",
      "|     B68| 300883|2015-12-30|         0.0|      0.0|      1.0|\n",
      "|     B83| 306257|2015-12-30|         0.0|      0.0|      1.0|\n",
      "|     B44| 303459|2015-12-30|         1.0|      1.0|      1.0|\n",
      "|     B82| 300476|2015-12-30|         1.0|      1.0|      1.0|\n",
      "|    B44+| 801163|2015-12-30|         1.0|      1.0|      1.0|\n",
      "|     B35| 302750|2015-12-30|         0.0|      0.0|      1.0|\n",
      "|     B49| 304005|2015-12-30|         0.0|      0.0|      1.0|\n",
      "|     B82| 307535|2015-12-30|         1.0|      1.0|      1.0|\n",
      "|     B69| 305811|2015-12-30|         1.0|      1.0|      1.0|\n",
      "|     B68| 307942|2015-12-30|         0.0|      1.0|      1.0|\n",
      "|     B43| 303722|2015-12-30|         1.0|      1.0|      1.0|\n",
      "|     B12| 301407|2015-12-30|         1.0|      1.0|      1.0|\n",
      "|     B70| 307647|2015-12-30|         1.0|      1.0|      1.0|\n",
      "|     B47| 303193|2015-12-30|         0.0|      1.0|      1.0|\n",
      "|     B52| 307596|2015-12-30|         1.0|      1.0|      1.0|\n",
      "|     B46| 303698|2015-12-30|         0.0|      0.0|      1.0|\n",
      "+--------+-------+----------+------------+---------+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new = sqlContext.sql('SELECT ROUTE_ID,STOP_ID, date,\\\n",
    "                      COUNT(IF((delay BETWEEN -60 AND 300),1,null))/COUNT(delay) as ontime_ratio,\\\n",
    "                      COUNT(IF((HOUR(realtime) BETWEEN 6 AND 9) OR (HOUR(realtime) BETWEEN 16 AND 19) AND (delay BETWEEN -300 AND 300),1,null))/COUNT(IF((HOUR(realtime) BETWEEN 6 AND 9) OR (HOUR(realtime) BETWEEN 16 AND 19),1,null)) as peak_wait,\\\n",
    "                      COUNT(IF((HOUR(realtime) NOT BETWEEN 6 AND 9) OR (HOUR(realtime) NOT BETWEEN 16 AND 19) AND (delay BETWEEN -300 AND 300),1,null))/COUNT(IF((HOUR(realtime) NOT BETWEEN 6 AND 9) OR (HOUR(realtime) NOT BETWEEN 16 AND 19),1,null)) as peak_wait\\\n",
    "                      FROM new_join\\\n",
    "                      GROUP BY ROUTE_ID, STOP_ID, date').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Tansfer to UnixTimeStamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the trips of each line of everyday to test the data intergrety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gaps = sqlContext.sql('SELECT Route_Id, tripdate, count(recorded_time) AS trips\\\n",
    "                       FROM record\\\n",
    "                       GROUP BY Route_Id, tripdate\\\n",
    "                       ORDER BY tripdate DESC') #apply sql Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
