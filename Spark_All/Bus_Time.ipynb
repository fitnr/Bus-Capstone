{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This project parses 3 TB nested Json file into csv using pyspark along with sparksql for further analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Json, parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from scipy.interpolate import interp1d # import dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read JSON File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bus_file='test.jsons'\n",
    "bus = sqlContext.read.json(bus_file)\n",
    "bus.registerTempTable(\"bus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bus.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load and apply SQL Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open(\"spark_extract.sql\") as fr:\n",
    "     query = fr.read()\n",
    "output = sqlContext.sql(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten the list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def extract(parts):\n",
    "    for p in parts:\n",
    "        for o in itertools.izip(p.Line,p.Latitude,p.Longitude,p.RecordedAtTime,p.vehicleID,p.Trip,p.TripDate):\n",
    "            yield o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse_list(p):\n",
    "    if p.ROUTE_ID!=None:\n",
    "        return zip(p.ROUTE_ID,p.latitude,p.longitude,p.recorded_time\\\n",
    "                   ,p.vehicle_id,p.TRIP_ID,p.tripdate,p.SHAPE_ID\\\n",
    "                   ,p.STOP_ID,p.distance_stop,p.distance_shape,p.status,p.destination)\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tranfer time to Unix time for interpolatation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import dateutil.parser\n",
    "def unix_time(x):\n",
    "    dt = dateutil.parser.parse(x)\n",
    "    return time.mktime(dt.timetuple())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def findIncreasingList(parts):\n",
    "    prev = 0\n",
    "    for record in parts:\n",
    "        if record[-1]<prev:\n",
    "            return\n",
    "        prev = record[-1]\n",
    "        yield record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d # import dependency\n",
    "def predict(x):\n",
    "    pre_x = [p[-1] for p in x if p[-1]!=None]\n",
    "    if len(pre_x) >= 2:\n",
    "        pre_y = [unix_time(p[1]) for p in x if p[-1]!=None]\n",
    "        f = interp1d(pre_x, pre_y)\n",
    "    else:\n",
    "        return []\n",
    "    return findIncreasingList([(p[0],p[2],p[3],f(p[-1]+p[-2]))\\\n",
    "                               for p in x if p[-1]!=None and (p[-1]+p[-2]) <= pre_x[-1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## method b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "def predict_map(x):\n",
    "    train_y = [unix_time(p[3]) for p in x if p[-3]!=None ]\n",
    "    if len(train_y) >= 2:\n",
    "        train_x = [p[-3] for p in x if p[-3]!=None]\n",
    "        f = interp1d(train_x, train_y)\n",
    "        distance = [(p[-3]+p[-4]) for p in x \\\n",
    "                    if p[-3]!=None and (p[-3]+p[-4]) <= train_x[-1]]\n",
    "        stoptimes = f(distance)\n",
    "        stops = [p[-5] for p in x if p[-3]!=None]\n",
    "    else:\n",
    "        return[]\n",
    "    return map(lambda a,b: (a,b), stops,stoptimes)\n",
    "    #return [(p[-4],f(p[-2]+p[-3])) for p in x if (p[-2]!=None and p[-3]!=None) and (p[-2]+p[-3]) <= pre_x[-1]] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupby Date and Line & Apply Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output.flatMap(parse_list)\\\n",
    "      .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group By TRIP_ID and Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.resultiterable.ResultIterable at 0x112c96d10>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112c96c90>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112c96150>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112c96a10>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112c96f90>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112c96e10>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112c95a10>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112c959d0>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112c95c90>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112c95050>]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.flatMap(parse_list)\\\n",
    "      .map(lambda x:((x[5],x[6]),x)).groupByKey()\\\n",
    "      .map(lambda x: x[1])\\\n",
    "      .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupbykey and Apply Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<pyspark.resultiterable.ResultIterable at 0x112ceff50>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112d291d0>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112d292d0>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112d29390>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112d29510>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112d29690>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112d29810>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112d29990>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112d29b10>,\n",
       " <pyspark.resultiterable.ResultIterable at 0x112d29c10>]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.flatMap(parse_list)\\\n",
    "      .map(lambda x:((x[5],x[6]),x)).groupByKey() \\\n",
    "      .map(lambda x: x[1]).take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = max(sorted(output.flatMap(parse_list)\\\n",
    "      .map(lambda x:((x[5],x[6]),x)).groupByKey().mapValues(lambda x: len(x)).map(lambda x: x[1]).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'MTA NYCT_Q46',\n",
       "  u'MTA NYCT_QV_W5-Weekday-110900_Q46_6',\n",
       "  u'MTA_502327',\n",
       "  array(1451520016.346892)),\n",
       " (u'MTA NYCT_Q46',\n",
       "  u'MTA NYCT_QV_W5-Weekday-110900_Q46_6',\n",
       "  u'MTA_502331',\n",
       "  array(1451520119.0)),\n",
       " (u'MTA NYCT_Q20A',\n",
       "  u'MTA NYCT_CS_W5-Weekday-113900_MISC_769',\n",
       "  u'MTA_505023',\n",
       "  array(1451520122.033537)),\n",
       " (u'MTA NYCT_Q20A',\n",
       "  u'MTA NYCT_CS_W5-Weekday-113900_MISC_769',\n",
       "  u'MTA_505024',\n",
       "  array(1451520122.0558827)),\n",
       " (u'MTA NYCT_Q20A',\n",
       "  u'MTA NYCT_CS_W5-Weekday-113900_MISC_769',\n",
       "  u'MTA_505026',\n",
       "  array(1451520338.3900702)),\n",
       " (u'MTA NYCT_BX8',\n",
       "  u'MTA NYCT_WF_W5-Weekday-110800_BX8_13',\n",
       "  u'MTA_100947',\n",
       "  array(1451520014.054865)),\n",
       " (u'MTA NYCT_BX8',\n",
       "  u'MTA NYCT_WF_W5-Weekday-110800_BX8_13',\n",
       "  u'MTA_103181',\n",
       "  array(1451520125.8062081)),\n",
       " (u'MTA NYCT_BX8',\n",
       "  u'MTA NYCT_WF_W5-Weekday-110800_BX8_13',\n",
       "  u'MTA_103183',\n",
       "  array(1451520377.7902856)),\n",
       " (u'MTA NYCT_BX10',\n",
       "  u'MTA NYCT_KB_W5-Weekday-111000_BX10_32',\n",
       "  u'MTA_103960',\n",
       "  array(1451519992.9190516)),\n",
       " (u'MTA NYCT_BX10',\n",
       "  u'MTA NYCT_KB_W5-Weekday-111000_BX10_32',\n",
       "  u'MTA_103966',\n",
       "  array(1451520201.7349436))]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.flatMap(parse_list)\\\n",
    "      .map(lambda x:((x[5],x[6]),(x[0],x[3],x[5],x[8],x[-4],x[-3])))\\\n",
    "      .groupByKey()\\\n",
    "      .flatMap(lambda x: predict(x[1]))\\\n",
    "      .take(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove the prefix, timezones and save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output.flatMap(parse_list)\\\n",
    "      .map(lambda x:((x[5],x[6]),(x[0],)).groupByKey() \\\n",
    "      .flatMap(lambda x: predict(x[1]))\\\n",
    "      .map(lambda x: \",\".join(map(str, x)))\\\n",
    "      .map(lambda x: x.replace('MTA NYCT_', '').replace('MTABC_','').replace('MTA_','').replace('-05:00',''))\\\n",
    "      .saveAsTextFile('stoptimes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read From CSV and SQL Manupilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reset Schemas and Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "customSchema = StructType([StructField(\"ROUTE_ID\", StringType(), True),\\\n",
    "                           StructField(\"latitude\", DoubleType(), True),\\\n",
    "                           StructField(\"longitude\", DoubleType(), True),\\\n",
    "                           StructField(\"recorded_time\", StringType(), True),\\\n",
    "                           StructField(\"vehicle_id\", StringType(), True),\\\n",
    "                           StructField(\"TRIP_ID\", StringType(), True),\\\n",
    "                           StructField(\"tripdate\", DateType(), True),\\\n",
    "                           StructField(\"SHAPE_ID\", StringType(), True),\\\n",
    "                           StructField(\"STOP_ID\", StringType(), True),\\\n",
    "                           StructField(\"distance_stop\", StringType(), True),\\\n",
    "                           StructField(\"distance_shape\", StringType(), True),\\\n",
    "                           StructField(\"status\", StringType(), True),\\\n",
    "                           StructField(\"destination\", StringType(), True)])             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stop_times_schema = StructType([StructField(\"trip_id\", StringType(), True),\\\n",
    "                           StructField(\"arrival_time\", StringType(), True),\\\n",
    "                           StructField(\"departure_time\", StringType(), True),\\\n",
    "                           StructField(\"stop_id\", StringType(), True),\\\n",
    "                           StructField(\"stop_sequence\", IntegerType(), True),\\\n",
    "                           StructField(\"pickup_type\", IntegerType(), True),\n",
    "                           StructField(\"drop_off_type\", IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_stoptimes_schema = StructType([StructField(\"ROUTE_ID\", StringType(), True),\\\n",
    "                           StructField(\"TRIP_ID\", StringType(), True),\\\n",
    "                           StructField(\"STOP_ID\", StringType(), True),\\\n",
    "                           StructField(\"time\",IntegerType(), True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use CSV=>DF tool to read saved csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_stoptimes = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('stops.csv', schema = real_stoptimes_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stoptimes = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('stop_times.txt',schema = stop_times_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------+----------+\n",
      "|ROUTE_ID|             TRIP_ID|STOP_ID|      time|\n",
      "+--------+--------------------+-------+----------+\n",
      "|     Q46|QV_W5-Weekday-110...| 502327|1451520016|\n",
      "|     Q46|QV_W5-Weekday-110...| 502331|1451520119|\n",
      "|    Q20A|CS_W5-Weekday-113...| 505023|1451520122|\n",
      "|    Q20A|CS_W5-Weekday-113...| 505024|1451520122|\n",
      "|    Q20A|CS_W5-Weekday-113...| 505026|1451520338|\n",
      "|     BX8|WF_W5-Weekday-110...| 100947|1451520014|\n",
      "|     BX8|WF_W5-Weekday-110...| 103181|1451520126|\n",
      "|     BX8|WF_W5-Weekday-110...| 103183|1451520378|\n",
      "|    BX10|KB_W5-Weekday-111...| 103960|1451519993|\n",
      "|    BX10|KB_W5-Weekday-111...| 103966|1451520202|\n",
      "|    BX10|KB_W5-Weekday-111...| 103962|1451520342|\n",
      "|    BX10|KB_W5-Weekday-111...| 103954|1451520347|\n",
      "|     B54|FP_W5-Weekday-112...| 306929|1451520018|\n",
      "|     B54|FP_W5-Weekday-112...| 306927|1451520130|\n",
      "|     B54|FP_W5-Weekday-112...| 304391|1451520300|\n",
      "|     B54|FP_W5-Weekday-112...| 304392|1451520333|\n",
      "|      B6|UP_W5-Weekday-109...| 300628|1451519990|\n",
      "|      B6|UP_W5-Weekday-109...| 300632|1451520146|\n",
      "|     M98|MV_W5-Weekday-108...| 403417|1451520019|\n",
      "|     M98|MV_W5-Weekday-108...| 403418|1451520286|\n",
      "+--------+--------------------+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "real_stoptimes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------+-------+-------------+-----------+-------------+\n",
      "|             trip_id|arrival_time|departure_time|stop_id|stop_sequence|pickup_type|drop_off_type|\n",
      "+--------------------+------------+--------------+-------+-------------+-----------+-------------+\n",
      "|EN_U5-Weekday-008...|    01:20:00|      01:20:00| 901471|            1|          0|            0|\n",
      "|EN_U5-Weekday-008...|    01:21:28|      01:21:28| 301386|            2|          0|            0|\n",
      "|EN_U5-Weekday-008...|    01:22:00|      01:22:00| 301387|            3|          0|            0|\n",
      "|EN_U5-Weekday-008...|    01:22:55|      01:22:55| 301388|            4|          0|            0|\n",
      "|EN_U5-Weekday-008...|    01:23:13|      01:23:13| 301389|            5|          0|            0|\n",
      "|EN_U5-Weekday-008...|    01:23:53|      01:23:53| 301390|            6|          0|            0|\n",
      "|EN_U5-Weekday-008...|    01:24:50|      01:24:50| 307120|            7|          0|            0|\n",
      "|EN_U5-Weekday-008...|    01:26:00|      01:26:00| 301392|            8|          0|            0|\n",
      "|EN_U5-Weekday-008...|    01:26:29|      01:26:29| 308274|            9|          0|            0|\n",
      "|EN_U5-Weekday-008...|    01:28:07|      01:28:07| 306348|           10|          0|            0|\n",
      "|EN_U5-Weekday-008...|    01:28:48|      01:28:48| 307309|           11|          0|            0|\n",
      "|EN_U5-Weekday-008...|    01:29:38|      01:29:38| 301395|           12|          0|            0|\n",
      "|EN_U5-Weekday-008...|    01:30:03|      01:30:03| 301397|           13|          0|            0|\n",
      "|EN_U5-Weekday-008...|    01:31:02|      01:31:02| 301398|           14|          0|            0|\n",
      "|EN_U5-Weekday-008...|    01:32:00|      01:32:00| 306438|           15|          0|            0|\n",
      "|EN_U5-Weekday-008...|    01:32:55|      01:32:55| 301400|           16|          0|            0|\n",
      "|EN_U5-Weekday-008...|    01:34:09|      01:34:09| 301402|           17|          0|            0|\n",
      "|EN_U5-Weekday-008...|    01:35:02|      01:35:02| 301403|           18|          0|            0|\n",
      "|EN_U5-Weekday-008...|    01:35:34|      01:35:34| 301404|           19|          0|            0|\n",
      "|EN_U5-Weekday-008...|    01:36:00|      01:36:00| 301405|           20|          0|            0|\n",
      "+--------------------+------------+--------------+-------+-------------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stoptimes.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stoptimes.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "record = sqlContext.read.format('com.databricks.spark.csv').options(header='true').load('all.csv', schema = customSchema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "record.registerTempTable('record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_time = real_stoptimes.withColumn('realtime',split(pyspark.sql.functions.from_unixtime(real_stoptimes.time), ' ')[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------+----------+--------+\n",
      "|ROUTE_ID|             TRIP_ID|STOP_ID|      time|realtime|\n",
      "+--------+--------------------+-------+----------+--------+\n",
      "|     Q46|QV_W5-Weekday-110...| 502327|1451520016|19:00:16|\n",
      "|     Q46|QV_W5-Weekday-110...| 502331|1451520119|19:01:59|\n",
      "+--------+--------------------+-------+----------+--------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_time.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_time.registerTempTable('new_time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stoptimes.registerTempTable('stoptimes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "def get_sec(s):\n",
    "    l = s.split(':')\n",
    "    return int(l[0]) * 3600 + int(l[1]) * 60 + int(l[2])\n",
    "sqlContext.registerFunction(\"getsec\", lambda x: get_sec(x), IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "join = sqlContext.sql('SELECT ROUTE_ID,TRIP_ID,STOP_ID,time,(getsec(realtime)-getsec(arrival_time)) as delay\\\n",
    "                       FROM new_time\\\n",
    "                       INNNER JOIN stoptimes\\\n",
    "                       ON (TRIP_ID = trip_id AND STOP_ID = stop_id)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-------+----------+-----+\n",
      "|ROUTE_ID|             TRIP_ID|STOP_ID|      time|delay|\n",
      "+--------+--------------------+-------+----------+-----+\n",
      "|     B41|FB_W5-Weekday-109...| 303241|1451520199|  754|\n",
      "|     B49|FB_W5-Weekday-110...| 303985|1451520386| 1046|\n",
      "+--------+--------------------+-------+----------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "join.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "join.registerTempTable('new_join')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|ROUTE_ID|       ontime_ratio|\n",
      "+--------+-------------------+\n",
      "|     Q24| 0.1111111111111111|\n",
      "|     B31| 0.7272727272727273|\n",
      "|     B32| 0.6666666666666666|\n",
      "|     B35|           0.265625|\n",
      "|     B36| 0.3333333333333333|\n",
      "|     B37|               0.75|\n",
      "|     B38| 0.4727272727272727|\n",
      "|     B39|                1.0|\n",
      "|      B1|0.27586206896551724|\n",
      "|      B2|                0.5|\n",
      "|      B3|0.11764705882352941|\n",
      "|      B4|0.23529411764705882|\n",
      "|      B6|0.30578512396694213|\n",
      "|      B7| 0.5714285714285714|\n",
      "|      B8|               0.26|\n",
      "|      B9|               0.35|\n",
      "|     B41|0.18181818181818182|\n",
      "|     B42|                1.0|\n",
      "|     B43| 0.6216216216216216|\n",
      "|     B44|0.45652173913043476|\n",
      "+--------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new = sqlContext.sql('SELECT ROUTE_ID, COUNT(IF((delay BETWEEN -60 AND 300),1,null))/COUNT(delay) as ontime_ratio\\\n",
    "                      FROM new_join\\\n",
    "                      GROUP BY ROUTE_ID').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Tansfer to UnixTimeStamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the trips of each line of everyday to test the data intergrety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gaps = sqlContext.sql('SELECT Route_Id, tripdate, count(recorded_time) AS trips\\\n",
    "                       FROM record\\\n",
    "                       GROUP BY Route_Id, tripdate\\\n",
    "                       ORDER BY tripdate DESC') #apply sql Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gaps.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step _ Merge Data from Stoptimes to interpolated times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combine = sqlContext.sql(\n",
    "    'SELECT record.TripRef, record.Stop_ID, record.RecordedAtTime \\\n",
    "    FROM record \\\n",
    "    JOIN stop_times \\\n",
    "    on (record.Stop_ID = stop_times.stop_id AND record.TripRef = stop_times.trip_id)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#output_df = output1.toDF(['Line','Lat','Lon','Recordtime','ID','Trip','TripDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#output_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
